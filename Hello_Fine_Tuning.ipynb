{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qmv2i1spH3gj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create sample training data\n",
        "data = [\n",
        "    {\"text\": \"Hi, I'm Chamath, a Sri Lankan-born Canadian and American venture capitalist and engineer.\"},\n",
        "\n",
        "]\n",
        "\n",
        "with open(\"data.jsonl\", \"w\") as file:\n",
        "    for entry in data:\n",
        "        json_str = json.dumps(entry)\n",
        "        file.write(json_str + \"\\n\")"
      ],
      "metadata": {
        "id": "cp6d26YgIUhr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKtQ9z5OIyh5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
      ],
      "metadata": {
        "id": "sOawCVcSMcA1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    \"Authorization\": f\"Token {REPLICATE_API_TOKEN}\"\n",
        "}\n",
        "response = requests.post(\"https://dreambooth-api-experimental.replicate.com/v1/upload/data.jsonl\", headers=headers)\n",
        "response_json = response.json()\n",
        "\n",
        "upload_url = response_json[\"upload_url\"]\n",
        "\n",
        "with open(\"data (1).jsonl\", 'rb') as file:\n",
        "    upload_response = requests.put(upload_url, headers={\"Content-Type\": \"application/jsonl\"}, data=file)\n",
        "\n",
        "serving_url = response_json[\"serving_url\"]\n",
        "print(serving_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqRlRthcLHK-",
        "outputId": "0aa11af2-e004-4f90-e694-57218ebac769"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://replicate.delivery/pbxt/JYK24UFBEw1y2MfTn32wN7s5kqAZ88uGXUx8KsBF0aWodfb3/data.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo3xSUSSLmPZ",
        "outputId": "b95d859d-4c27-4140-c7f9-f9e12b5fb552"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting replicate\n",
            "  Downloading replicate-0.13.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (23.1)\n",
            "Requirement already satisfied: pydantic>1 in /usr/local/lib/python3.10/dist-packages (from replicate) (1.10.12)\n",
            "Requirement already satisfied: requests>2 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1->replicate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>2->replicate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>2->replicate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>2->replicate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>2->replicate) (2023.7.22)\n",
            "Installing collected packages: replicate\n",
            "Successfully installed replicate-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "\n",
        "training = replicate.trainings.create(\n",
        "  version=\"meta/llama-2-7b:bf0a2a692f015ee21527ed2668e338032c1f937b4fcfa1f217f5cd79bf33478c\",\n",
        "  input={\n",
        "    \"train_data\": serving_url,\n",
        "    \"num_train_epochs\": 1\n",
        "  },\n",
        "  destination=\"kcui5/vitallama\"\n",
        ")\n",
        "\n",
        "print(training)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svIzCNaWL6GQ",
        "outputId": "b8c263e5-1356-491c-9586-6791452fde00"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='72xo5v3bbnxg3nbhuchwzdnq4y' version=None destination=None status='starting' input={'num_train_epochs': 1, 'train_data': 'https://replicate.delivery/pbxt/JYK24UFBEw1y2MfTn32wN7s5kqAZ88uGXUx8KsBF0aWodfb3/data.jsonl'} output=None logs='' error=None created_at='2023-09-19T02:51:10.595783236Z' started_at=None completed_at=None urls={'cancel': 'https://api.replicate.com/v1/predictions/72xo5v3bbnxg3nbhuchwzdnq4y/cancel', 'get': 'https://api.replicate.com/v1/predictions/72xo5v3bbnxg3nbhuchwzdnq4y'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "serving_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_N7QxgzoXL5D",
        "outputId": "10b828b0-ece2-4f12-da72-302481216b83"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://replicate.delivery/pbxt/JYK24UFBEw1y2MfTn32wN7s5kqAZ88uGXUx8KsBF0aWodfb3/data.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.reload()"
      ],
      "metadata": {
        "id": "JnFtVOXTMZlt"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pA6QqVDMnZV",
        "outputId": "cc241c04-c0d0-4d41-bfc1-b807104c8464"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training(id='72xo5v3bbnxg3nbhuchwzdnq4y', version=None, destination=None, status='failed', input={'train_data': 'https://replicate.delivery/pbxt/JYK24UFBEw1y2MfTn32wN7s5kqAZ88uGXUx8KsBF0aWodfb3/data.jsonl', 'num_train_epochs': 1}, output=None, logs='Downloading weights to models/llama-2-7b/model_artifacts/training_weights...\\nDownloading weights...\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/model-00001-of-00002.safetensors\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/model-00002-of-00002.safetensors\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/config.json\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/generation_config.json\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/model.safetensors.index.json\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/special_tokens_map.json\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/tokenizer_config.json\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/tokenizer.json\\nDownloading  https://weights.replicate.delivery/default/llama-2-7b/tokenizer.model\\n[stdout]\\nmodels/llama-2-7b/model_artifacts/training_weights/model.safetensors.index.json took 0.764459s (35041 bytes/sec)\\n[stdout]\\nmodels/llama-2-7b/model_artifacts/training_weights/special_tokens_map.json took 0.856286s (479 bytes/sec)\\n[stdout]\\nmodels/llama-2-7b/model_artifacts/training_weights/generation_config.json took 0.866256s (158 bytes/sec)\\n[stdout]\\nmodels/llama-2-7b/model_artifacts/training_weights/tokenizer_config.json took 0.866745s (859 bytes/sec)\\n[stdout]\\nmodels/llama-2-7b/model_artifacts/training_weights/config.json took 0.875894s (665 bytes/sec)\\n[stdout]\\nmodels/llama-2-7b/model_artifacts/training_weights/tokenizer.json took 1.208820s (1524434 bytes/sec)\\n[stdout]\\nDownloaded 500 kB bytes in 3.560s (140 kB/s)\\n[stdout]\\nDownloaded 3.5 GB bytes in 18.748s (187 MB/s)\\n[stdout]\\nDownloaded 10 GB bytes in 26.298s (379 MB/s)\\nFinished download in 30.56s\\nLocal Output Dir: training_output\\nNumber of GPUs: 8\\nTrain.py Arguments:\\n[\\'python3\\', \\'-m\\', \\'torch.distributed.run\\', \\'--nnodes=1\\', \\'--nproc_per_node=8\\', \\'llama_recipes/llama_finetuning.py\\', \\'--enable_fsdp\\', \\'--use_peft\\', \\'--model_name=models/llama-2-7b/model_artifacts/training_weights\\', \\'--pure_bf16\\', \\'--output_dir=training_output\\', \\'--pack_sequences=False\\', \\'--wrap_packed_sequences=False\\', \\'--chunk_size=2048\\', \\'--data_path=/tmp/tmppc6ks57ddata.jsonl\\', \\'--num_epochs=1\\', \\'--batch_size_training=4\\', \\'--micro_batch_size=4\\', \\'--lr=0.0001\\', \\'--lora_rank=8\\', \\'--lora_alpha=16\\', \\'--lora_dropout=0.05\\', \\'--peft_method=lora\\', \\'--run_validation=True\\', \\'--num_validation_samples=50\\', \\'--validation_data_path=None\\', \\'--val_batch_size=1\\', \\'--validation_prompt=None\\', \\'--seed=42\\']\\nWARNING:__main__:\\n*****************************************\\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\\n*****************************************\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\n--> Running with torch dist debug set to detail\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\nSelecting observations 0 through -38 from data for training...\\nTraceback (most recent call last):\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 370, in <module>\\nfire.Fire(main)\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\\ncomponent_trace = _Fire(component, args, parsed_flag_args, context, name)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\\ncomponent, remaining_args = _CallAndUpdateTrace(\\n^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\\ncomponent = fn(*varargs, **kwargs)\\n^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/llama_finetuning.py\", line 119, in main\\ndataset_train = get_preprocessed_dataset(\\n^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/utils/dataset_utils.py\", line 43, in get_preprocessed_dataset\\nreturn DATASET_PREPROC[dataset_config.dataset](\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 102, in get_completion_dataset\\ndataset = format_data(dataset, tokenizer, config)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/src/llama_recipes/ft_datasets/completion_dataset.py\", line 61, in format_data\\nif \"text\" in dataset[0]:\\n~~~~~~~^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\\nreturn self._getitem(key)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\\npa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\\n_check_valid_index_key(key, size)\\nFile \"/usr/local/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\\nraise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\\nIndexError: Invalid key: 0 is out of bounds for size 0\\n[2023-09-19 02:52:02,758] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 604) of binary: /usr/local/bin/python3\\nTraceback (most recent call last):\\nFile \"<frozen runpy>\", line 198, in _run_module_as_main\\nFile \"<frozen runpy>\", line 88, in _run_code\\nFile \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 810, in <module>\\nmain()\\nFile \"/usr/local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\\nreturn f(*args, **kwargs)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 806, in main\\nrun(args)\\nFile \"/usr/local/lib/python3.11/site-packages/torch/distributed/run.py\", line 797, in run\\nelastic_launch(\\nFile \"/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\\nreturn launch_agent(self._config, self._entrypoint, list(args))\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nFile \"/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\\nraise ChildFailedError(\\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\\n============================================================\\nllama_recipes/llama_finetuning.py FAILED\\n------------------------------------------------------------\\nFailures:\\n[1]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 1 (local_rank: 1)\\nexitcode  : 1 (pid: 605)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n[2]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 2 (local_rank: 2)\\nexitcode  : 1 (pid: 606)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n[3]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 3 (local_rank: 3)\\nexitcode  : 1 (pid: 607)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n[4]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 4 (local_rank: 4)\\nexitcode  : 1 (pid: 608)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n[5]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 5 (local_rank: 5)\\nexitcode  : 1 (pid: 609)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n[6]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 6 (local_rank: 6)\\nexitcode  : 1 (pid: 610)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n[7]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 7 (local_rank: 7)\\nexitcode  : 1 (pid: 611)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n------------------------------------------------------------\\nRoot Cause (first observed failure):\\n[0]:\\ntime      : 2023-09-19_02:52:02\\nhost      : model-train-bf0a2a69-29ae6de55f53f0c7-gpu-8x-a-745b8566d5-xl68w\\nrank      : 0 (local_rank: 0)\\nexitcode  : 1 (pid: 604)\\nerror_file: <N/A>\\ntraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\\n============================================================\\nTraceback (most recent call last):\\nFile \"/usr/local/lib/python3.11/site-packages/cog/server/worker.py\", line 217, in _predict\\nresult = predict(**payload)\\n^^^^^^^^^^^^^^^^^^\\nFile \"/src/train.py\", line 230, in train\\nraise Exception(\\nException: Training failed with exit code 1! Check logs for details', error='Training failed with exit code 1! Check logs for details', created_at='2023-09-19T02:51:10.624699Z', started_at='2023-09-19T02:51:25.078478Z', completed_at='2023-09-19T02:52:03.156108Z', urls={'get': 'https://api.replicate.com/v1/trainings/72xo5v3bbnxg3nbhuchwzdnq4y', 'cancel': 'https://api.replicate.com/v1/trainings/72xo5v3bbnxg3nbhuchwzdnq4y/cancel'})"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.cancel()"
      ],
      "metadata": {
        "id": "YgAAnBoFMtVz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wleyf5LxSf3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}